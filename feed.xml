<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://weitunglin.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://weitunglin.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-06-30T05:29:10+00:00</updated><id>https://weitunglin.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Mamba in Computer Vision</title><link href="https://weitunglin.github.io/blog/2024/mamba-vision/" rel="alternate" type="text/html" title="Mamba in Computer Vision"/><published>2024-05-10T00:00:00+00:00</published><updated>2024-05-10T00:00:00+00:00</updated><id>https://weitunglin.github.io/blog/2024/mamba-vision</id><content type="html" xml:base="https://weitunglin.github.io/blog/2024/mamba-vision/"><![CDATA[<h2 id="tldr">TL;DR</h2> <p>Vim and VMamba were the first to apply Mamba to Computer Vision. (The two papers came out almost simultaneously, just one day apart. Although Vim was submitted to a conference, VMamba’s GitHub community is very active and shows better results. If you want to get started, VMamba is recommended.)</p> <p>Vim is very similar to ViT (Block<em>24, homography), mainly changing the token scan from a simple 1D flatten to 1D flatten plus flipped 1D flatten. VMamba is very similar to Swin-T (Stage</em>4, hierarchy), with token scan becoming 4 types: horizontal (forward+backward) + vertical (forward+backward). LocalMamba introduces a windowed concept for scanning, allowing the model to learn itself (7 choices in total). It shows good improvements when added to both Vim and VMamba. EfficientVMamba proposes Atrous scan, which can be understood as skipping one token between every two scanned tokens.</p> <hr/> <p>Vim 跟 VMamba 首先提出將 Mamba 用於 Computer Vision 領域。（兩篇論文幾乎同時，只差一天，雖然 Vim 已投稿上會議，但 VMamba GitHub 社群非常活躍，效果也比較好，如果要上手會推薦 VMamba）</p> <ul> <li>Vim 與 ViT (Block*24, homography) 非常相似，主要在 token scan 中由單純 1d flatten 轉為 1d flatten 加上 flipped 1d flatten。</li> <li>VMamba 與 Swin-T (Stage*4, hierarchy) 非常相似，token scan 變成 4 種: horizontal (forward+backward) + vertical (forward+backward)。</li> <li>LocalMamba 針對 scan 的方法提出 windowed 的概念並讓模型自己學習 (共有 7 種選擇)，加在 Vim 與 VMamba 中皆有不錯的提升。</li> <li>EfficientVMamba 提出 Atrous scan，可以理解成 scan 兩個 token 之間 skip 一個 token。</li> </ul> <h2 id="presentation">Presentation</h2> <iframe src="https://docs.google.com/presentation/d/e/2PACX-1vTR-vVTsWMzrNyL2hMQGG3uIxF05lhaKQjq0baTJPNU3EiumVA5dsc5iMe4C7oA7qA1P2L_NmM0b2fA/embed?start=false&amp;loop=false&amp;delayms=60000" frameborder="0" width="900" height="675" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>]]></content><author><name></name></author><category term="paper-review"/><category term="computer-vision"/><category term="mamba"/><summary type="html"><![CDATA[An Introduction to Mamba in Computer Vision]]></summary></entry><entry><title type="html">Mamba: Linear-Time Sequence Modeling with Selective State Spaces</title><link href="https://weitunglin.github.io/blog/2024/mamba/" rel="alternate" type="text/html" title="Mamba: Linear-Time Sequence Modeling with Selective State Spaces"/><published>2024-03-22T00:00:00+00:00</published><updated>2024-03-22T00:00:00+00:00</updated><id>https://weitunglin.github.io/blog/2024/mamba</id><content type="html" xml:base="https://weitunglin.github.io/blog/2024/mamba/"><![CDATA[<h2 id="tldr">TL;DR</h2> <p>Mamba is not an attention mechanism, but rather an attention-free architecture for sequence modeling. It achieves linear time complexity with respect to sequence length (self-attention has quadratic complexity). Mamba builds upon State Space Models (SSMs) and can be viewed as an enhanced RNN with:</p> <ol> <li>Improved selectivity, allowing input-dependent parameter adjustments</li> <li>Faster training through the use of Parallel Prefix Sum algorithm</li> <li>Linear-time scaling for both training and inference</li> </ol> <p>This approach enables Mamba to efficiently handle long sequences and achieve state-of-the-art results across diverse domains including language, audio, and long-context tasks.</p> <hr/> <p>過去 4~5 年 transformer 在 deep learning 領域可以說是翻天覆地，把 CNN 的紀錄都刷了個遍。 而 transformer 底層技術 self-attention 有個最大的缺點：次方成長的複雜度（時間+空間）。 Mamba 完全沒有用 self-attention，達成了相同的 performace，而且是線性複雜度。 可以把 Mamba 想像成進化的 RNN：有更好的選擇狀態機制跟加快訓練時間 (PrefixParallelSum: n -&gt; log(n))</p> <h2 id="presentation">Presentation</h2> <iframe src="https://docs.google.com/presentation/d/e/2PACX-1vRkylHJ3uONFtnL6VLP0ENKiQ68llpYt9HhrNfo3x8GekIUd0Egz0E7zG_PD7xfAt8AYUujexUuXnMG/embed?start=false&amp;loop=false&amp;delayms=60000" frameborder="0" width="900" height="675" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>]]></content><author><name></name></author><category term="paper-review"/><category term="mamba"/><category term="ssm"/><summary type="html"><![CDATA[An Introduction to Mamba, an Alternative to self-attention]]></summary></entry></feed>